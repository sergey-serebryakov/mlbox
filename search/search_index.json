{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"MLCube \u00b6 Interested in getting started with MLCube? Follow the Getting Started instructions, or watch the video below. MLCube is a project that reduces friction for machine learning by ensuring that models are easily portable and reproducible, e.g., between different stacks such as different clouds, between cloud and on-prem, etc. Installing MLCube \u00b6 Install from PyPI: pip install mlcube To uninstall: pip uninstall mlcube Usage Examples \u00b6 Check out the examples for detailed examples. License \u00b6 MLCube is licensed under the Apache License 2.0. See LICENSE for more information. Support \u00b6 Create a GitHub issue","title":"Home"},{"location":"#mlcube","text":"Interested in getting started with MLCube? Follow the Getting Started instructions, or watch the video below. MLCube is a project that reduces friction for machine learning by ensuring that models are easily portable and reproducible, e.g., between different stacks such as different clouds, between cloud and on-prem, etc.","title":"MLCube"},{"location":"#installing-mlcube","text":"Install from PyPI: pip install mlcube To uninstall: pip uninstall mlcube","title":"Installing MLCube"},{"location":"#usage-examples","text":"Check out the examples for detailed examples.","title":"Usage Examples"},{"location":"#license","text":"MLCube is licensed under the Apache License 2.0. See LICENSE for more information.","title":"License"},{"location":"#support","text":"Create a GitHub issue","title":"Support"},{"location":"getting-started/","text":"Installation \u00b6 Here is the step-by-step guide to install MLCube library and run simple MLCube cubes. Create a python environment \u00b6 # Option 1: use python virtual environment `virtualenv`. virtualenv -p python3.6 ./env_mlcube && source ./env_mlcube/bin/activate # Option 2: use conda. conda create -n mlcube python=3.6 && conda activate mlcube Install MLCube Runners \u00b6 # Install MLCube Docker runner. pip install mlcube-docker # Optionally, install other runners. # pip install mlcube-gcp # pip install mlcube-k8s # pip install mlcube-kubeflow # pip install mlcube-singularity # pip install mlcube-ssh # Check that the docker runner has been installed. mlcube config --get runners # Show MLCube system settings. mlcube config --list # This system settings file (~/mlcube.yaml) configures local MLCube runners. Documentation # for MLCube runners describes each of these parameters in details. A typical first step for # enterprise environments that are usually behind a firewall is to configure proxy servers. # platforms: # docker: # env_args: # http_proxy: http://ADDRESS:PORT # https_proxy: https://ADDRESS:PORT # build_args: # http_proxy: http://ADDRESS:PORT # https_proxy: https://ADDRESS:PORT Explore with examples \u00b6 A great way to learn about MLCube is try out the example MLCube cubes located here . git clone https://github.com/mlcommons/mlcube_examples.git && cd ./mlcube_examples mlcube describe --mlcube ./mnist","title":"Installation"},{"location":"getting-started/#installation","text":"Here is the step-by-step guide to install MLCube library and run simple MLCube cubes.","title":"Installation"},{"location":"getting-started/#create-a-python-environment","text":"# Option 1: use python virtual environment `virtualenv`. virtualenv -p python3.6 ./env_mlcube && source ./env_mlcube/bin/activate # Option 2: use conda. conda create -n mlcube python=3.6 && conda activate mlcube","title":"Create a python environment"},{"location":"getting-started/#install-mlcube-runners","text":"# Install MLCube Docker runner. pip install mlcube-docker # Optionally, install other runners. # pip install mlcube-gcp # pip install mlcube-k8s # pip install mlcube-kubeflow # pip install mlcube-singularity # pip install mlcube-ssh # Check that the docker runner has been installed. mlcube config --get runners # Show MLCube system settings. mlcube config --list # This system settings file (~/mlcube.yaml) configures local MLCube runners. Documentation # for MLCube runners describes each of these parameters in details. A typical first step for # enterprise environments that are usually behind a firewall is to configure proxy servers. # platforms: # docker: # env_args: # http_proxy: http://ADDRESS:PORT # https_proxy: https://ADDRESS:PORT # build_args: # http_proxy: http://ADDRESS:PORT # https_proxy: https://ADDRESS:PORT","title":"Install MLCube Runners"},{"location":"getting-started/#explore-with-examples","text":"A great way to learn about MLCube is try out the example MLCube cubes located here . git clone https://github.com/mlcommons/mlcube_examples.git && cd ./mlcube_examples mlcube describe --mlcube ./mnist","title":"Explore with examples"},{"location":"getting-started/hello-world/","text":"Hello World \u00b6 Hello World MLCube is an example of a Docker-based cube. QuickStart \u00b6 Get started with MLCube Docker runner with below commands. Create python environment \u00b6 virtualenv -p python3.6 ./env && source ./env/bin/activate Install MLCube Docker runner \u00b6 pip install mlcube-docker # Install. mlcube config --get runners # Check it was installed. mlcube config --list # Show system settings for local MLCube runners. Depending on how your local system is configured, it may be required to change the following settings: - platforms.docker.docker (string): A docker executable. Examples are docker , nvidia-docker , sudo docker , podman etc. - platforms.docker.env_args (dictionary) and platforms.docker.build_args (dictionary). Environmental variables for docker run and build phases. Http and https proxy settings can be configured here. A custom configuration could look like: platforms : docker : docker : sudo docker env_args : http_proxy : http://proxy.company.com:8088 https_proxy : https://proxy.company.com.net:8088 build_args : http_proxy : http://proxy.company.com:8088 https_proxy : https://proxy.company.com:8088 Run Hello World MLCube example \u00b6 # The hello_world MLCube is part of the mlcube_examples GitHub repository. git clone https://github.com/mlcommons/mlcube_examples.git && cd ./mlcube_examples/hello_world # Run Hello World MLCube on a local machine with Docker runner. # Show available tasks mlcube describe # Run Hello World example tasks. Very first run can take some time to download (or build) # the MLCube docker image. mlcube run --mlcube=. --task=hello --platform=docker # No output expected. mlcube run --mlcube=. --task=bye --platform=docker # No output expected. cat ./workspace/chats/chat_with_alice.txt # You should some log lines in this file. cat If above mlcube runs fail (with the error message saying there is no docker image available, try to change the system settings file by changing platforms.docker.build_strategy to auto . Setup Docker \u00b6 MLCube Docker runner used Docker runtime, and they must be available in the system. Installation guides for various operating systems can be found here . This example was tested on a system where users are in the docker group and run docker without sudo . To add yourself to a docker group, run the following: sudo groupadd docker # Add the docker group if it doesn't already exist. sudo gpasswd -a ${USER} docker # Add the connected user \"${USER}\" to the docker group. Change # the user name to match your preferred user. sudo service docker restart # Restart the Docker daemon. newgrp docker # Either do a `newgrp docker` or log out/in to activate the # changes to groups. Configuring Hello World MLCube \u00b6 Cubes need to be configured before they can run. MLCube runners do that automatically, and users do not need to run the configure step manually. If for some reason this needs to be done, for instance, to pre-build or pull docker images (if these processes take too much time), MLCube runtime implements configure command. The Hello World cube is a Docker-based cube, and users can configure the MLCube by running the following command: mlcube configure --mlcube=. --platform=docker The Docker runner will build or will pull the docker image for the Hello World cube. As it is mentioned above, this step is optional and is only required when MLCubes need to be rebuilt. This can happen when users change implementation files and want to re-package their ML project into MLCube. In other situations, MLCube runners can auto-detect if configure command needs to be run before running MLCube tasks. Running Hello World MLCube \u00b6 In order to run the Hello World cube, users need to provide the path to the root directory of the cube, platform and task names. Run the following two commands one at a time: cat ./workspace/chats/chat_with_alice.txt mlcube run --mlcube=. --platform=docker --task=hello cat ./workspace/chats/chat_with_alice.txt mlcube run --mlcube=. --platform=docker --task=bye cat ./workspace/chats/chat_with_alice.txt Hello World creates a file workspace/chats/chat_with_alice.txt that contains the following: [2020-09-03 09:13:14.236945] Hi, Alice! Nice to meet you. [2020-09-03 09:13:20.749831] Bye, Alice! It was great talking to you. Modifying MLCube tasks \u00b6 Override parameters on a command line \u00b6 One way to change the parameters of MLCubes is to override them on a command line. Create a new file workspace/names/mary.txt with the following content: Mary . Then, run the following: mlcube run --mlcube = . --platform = docker --task = hello name = names/mary.txt chat = chats/chat_with_mary.txt cat workspace/chats/chat_with_mary.txt mlcube run --mlcube = . --platform = docker --task = bye name = names/mary.txt chat = chats/chat_with_mary.txt cat workspace/chats/chat_with_mary.txt You should observe the output similar to this one: [ 2021 -09-30 18 :49:46.896509 ] Hi, Mary! Nice to meet you. [ 2021 -09-30 18 :49:56.883266 ] Bye, Mary! It was great talking to you. Providing a better greeting message \u00b6 Because how Hello World cube was implemented, the greeting message is always the following: Nice to meet you. . We will update the implementation so that if this is not the first time Alice says hello , the MLCube will respond: Nice to see you again. . Modify the file hello_world.py . Update the function named get_greeting_message on line 14. It should have the following implementation: import os def get_greeting_message ( chat_file : str ) -> str : return \"Nice to meet you.\" if not os . path . exists ( chat_file ) else \"Nice to see you again.\" Reconfigure the MLCube: mlcube configure --mlcube=. --platform=docker And run two hello tasks again: rm ./workspace/chats/chat_with_alice.txt mlcube run --mlcube = . --platform = docker --task = hello,bye mlcube run --mlcube = . --platform = docker --task = hello,bye cat ./workspace/chats/chat_with_alice.txt The MLCube recognized it was not the first time it talked to Alice, and changed the greeting: [2021-09-30 20:04:36.977032] Hi, Alice! Nice to meet you. [2021-09-30 20:04:40.851157] Bye, Alice! It was great talking to you. [2021-09-30 20:04:47.228554] Hi, Alice! Nice to see you again. [2021-09-30 20:04:51.031609] Bye, Alice! It was great talking to you.","title":"Hello World"},{"location":"getting-started/hello-world/#hello-world","text":"Hello World MLCube is an example of a Docker-based cube.","title":"Hello World"},{"location":"getting-started/hello-world/#quickstart","text":"Get started with MLCube Docker runner with below commands.","title":"QuickStart"},{"location":"getting-started/hello-world/#create-python-environment","text":"virtualenv -p python3.6 ./env && source ./env/bin/activate","title":"Create python environment"},{"location":"getting-started/hello-world/#install-mlcube-docker-runner","text":"pip install mlcube-docker # Install. mlcube config --get runners # Check it was installed. mlcube config --list # Show system settings for local MLCube runners. Depending on how your local system is configured, it may be required to change the following settings: - platforms.docker.docker (string): A docker executable. Examples are docker , nvidia-docker , sudo docker , podman etc. - platforms.docker.env_args (dictionary) and platforms.docker.build_args (dictionary). Environmental variables for docker run and build phases. Http and https proxy settings can be configured here. A custom configuration could look like: platforms : docker : docker : sudo docker env_args : http_proxy : http://proxy.company.com:8088 https_proxy : https://proxy.company.com.net:8088 build_args : http_proxy : http://proxy.company.com:8088 https_proxy : https://proxy.company.com:8088","title":"Install MLCube Docker runner"},{"location":"getting-started/hello-world/#run-hello-world-mlcube-example","text":"# The hello_world MLCube is part of the mlcube_examples GitHub repository. git clone https://github.com/mlcommons/mlcube_examples.git && cd ./mlcube_examples/hello_world # Run Hello World MLCube on a local machine with Docker runner. # Show available tasks mlcube describe # Run Hello World example tasks. Very first run can take some time to download (or build) # the MLCube docker image. mlcube run --mlcube=. --task=hello --platform=docker # No output expected. mlcube run --mlcube=. --task=bye --platform=docker # No output expected. cat ./workspace/chats/chat_with_alice.txt # You should some log lines in this file. cat If above mlcube runs fail (with the error message saying there is no docker image available, try to change the system settings file by changing platforms.docker.build_strategy to auto .","title":"Run Hello World MLCube example"},{"location":"getting-started/hello-world/#setup-docker","text":"MLCube Docker runner used Docker runtime, and they must be available in the system. Installation guides for various operating systems can be found here . This example was tested on a system where users are in the docker group and run docker without sudo . To add yourself to a docker group, run the following: sudo groupadd docker # Add the docker group if it doesn't already exist. sudo gpasswd -a ${USER} docker # Add the connected user \"${USER}\" to the docker group. Change # the user name to match your preferred user. sudo service docker restart # Restart the Docker daemon. newgrp docker # Either do a `newgrp docker` or log out/in to activate the # changes to groups.","title":"Setup Docker"},{"location":"getting-started/hello-world/#configuring-hello-world-mlcube","text":"Cubes need to be configured before they can run. MLCube runners do that automatically, and users do not need to run the configure step manually. If for some reason this needs to be done, for instance, to pre-build or pull docker images (if these processes take too much time), MLCube runtime implements configure command. The Hello World cube is a Docker-based cube, and users can configure the MLCube by running the following command: mlcube configure --mlcube=. --platform=docker The Docker runner will build or will pull the docker image for the Hello World cube. As it is mentioned above, this step is optional and is only required when MLCubes need to be rebuilt. This can happen when users change implementation files and want to re-package their ML project into MLCube. In other situations, MLCube runners can auto-detect if configure command needs to be run before running MLCube tasks.","title":"Configuring Hello World MLCube"},{"location":"getting-started/hello-world/#running-hello-world-mlcube","text":"In order to run the Hello World cube, users need to provide the path to the root directory of the cube, platform and task names. Run the following two commands one at a time: cat ./workspace/chats/chat_with_alice.txt mlcube run --mlcube=. --platform=docker --task=hello cat ./workspace/chats/chat_with_alice.txt mlcube run --mlcube=. --platform=docker --task=bye cat ./workspace/chats/chat_with_alice.txt Hello World creates a file workspace/chats/chat_with_alice.txt that contains the following: [2020-09-03 09:13:14.236945] Hi, Alice! Nice to meet you. [2020-09-03 09:13:20.749831] Bye, Alice! It was great talking to you.","title":"Running Hello World MLCube"},{"location":"getting-started/hello-world/#modifying-mlcube-tasks","text":"","title":"Modifying MLCube tasks"},{"location":"getting-started/hello-world/#override-parameters-on-a-command-line","text":"One way to change the parameters of MLCubes is to override them on a command line. Create a new file workspace/names/mary.txt with the following content: Mary . Then, run the following: mlcube run --mlcube = . --platform = docker --task = hello name = names/mary.txt chat = chats/chat_with_mary.txt cat workspace/chats/chat_with_mary.txt mlcube run --mlcube = . --platform = docker --task = bye name = names/mary.txt chat = chats/chat_with_mary.txt cat workspace/chats/chat_with_mary.txt You should observe the output similar to this one: [ 2021 -09-30 18 :49:46.896509 ] Hi, Mary! Nice to meet you. [ 2021 -09-30 18 :49:56.883266 ] Bye, Mary! It was great talking to you.","title":"Override parameters on a command line"},{"location":"getting-started/hello-world/#providing-a-better-greeting-message","text":"Because how Hello World cube was implemented, the greeting message is always the following: Nice to meet you. . We will update the implementation so that if this is not the first time Alice says hello , the MLCube will respond: Nice to see you again. . Modify the file hello_world.py . Update the function named get_greeting_message on line 14. It should have the following implementation: import os def get_greeting_message ( chat_file : str ) -> str : return \"Nice to meet you.\" if not os . path . exists ( chat_file ) else \"Nice to see you again.\" Reconfigure the MLCube: mlcube configure --mlcube=. --platform=docker And run two hello tasks again: rm ./workspace/chats/chat_with_alice.txt mlcube run --mlcube = . --platform = docker --task = hello,bye mlcube run --mlcube = . --platform = docker --task = hello,bye cat ./workspace/chats/chat_with_alice.txt The MLCube recognized it was not the first time it talked to Alice, and changed the greeting: [2021-09-30 20:04:36.977032] Hi, Alice! Nice to meet you. [2021-09-30 20:04:40.851157] Bye, Alice! It was great talking to you. [2021-09-30 20:04:47.228554] Hi, Alice! Nice to see you again. [2021-09-30 20:04:51.031609] Bye, Alice! It was great talking to you.","title":"Providing a better greeting message"},{"location":"getting-started/mnist/","text":"MNIST \u00b6 The MNIST dataset is a collection of 60,000 handwritten digits widely used for training statistical, Machine Learning (ML) and Deep Learning (DL) models. The MNIST MLCube example demonstrates how data scientists, ML and DL researchers and developers can distribute their ML projects (including training, validation and inference code) as MLCube cubes. MLCube establishes a standard to package user workloads, and provides unified command line interface. In addition, MLCube provides a number of reference runners - python packages that can run cubes on different platforms including Docker , Singularity , KubeFlow and several others. A data scientist has been working on a machine learning project. The goal is to train a simple neural network to classify the collection of 60,000 small images into 10 classes. The source files for this MNIST example can be found on GitHub in MLCube Example repository . MNIST training code \u00b6 Training an ML model is a process involving multiple steps such as downloading data, analyzing and cleaning data, splitting data into train/validation/test data sets, running hyper-parameter optimization experiments and performing final model testing. MNIST dataset is a relatively small and well studied dataset that provides standard train/test split. In this simple example a developer needs to implement two steps - (1) downloading data and (2) training a model. We call these steps as tasks . Each task requires several parameters, such as a URL of the data set that we need to download, location on a local disk where the data set will be downloaded to, path to a directory that will contain training artifacts such as log files, training snapshots and Machine Learning models. We can characterize these two tasks in the following way: Download task: Inputs : A yaml file ( data_config ) with two parameters - dataset URI and dataset hash. Outputs : Directory to serialize the data set ( data_dir ) and directory to serialize log files ( log_dir ). Training task: Inputs : Directory with MNIST data set ( data_dir ), training hyper-parameters defined in a file ( train_config ). Outputs : Directory to store training results ( model_dir ) and directory to store log files ( log_dir ). We have intentionally made all input/output parameters to be file system artifacts. By doing so, we support reproducibility. Instead of command line arguments that can easily be lost, we store them in files. There are many ways to implement the MNIST example. For simplicity, we assume the following: We use one python file. Task name (download, train) is a command line positional parameter. Both tasks write logs, so it makes sense to add a parameter that defines a directory for log files. The download task accepts additional data directory parameter. The train task accepts such parameters as data and model directories, path to a file with hyper-parameter. Configurable hyper-parameters are: (1) optimizer name, (2) number of training epochs and (3) global batch size. Then, our implementation could look like this. Parse command line arguments and identify a task to run. If it is the download task, call a function that downloads data sets. If it is the train task, train a model. This is sort of single entrypoint implementation where we run one script asking to perform various tasks. We run our script (mnist.py) in the following way: python mnist.py download --data_config=PATH --data_dir=PATH --log_dir=PATH python mnist.py train --train_config=PATH --data_dir=PATH --model_dir=PATH --log_dir=PATH MLCube implementation \u00b6 Packaging our MNIST training script as a MLCube is done in several steps. We will be using a directory-based cube where a directory is structured in a certain way and contains specific files that make it MLCube compliant. We need to create an empty directory on a local disk. Let's assume we call it mnist and we'll use {MLCUBE_ROOT} to denote a full path to this directory. This is called an MLCube root directory. At this point this directory is empty: mnist/ Build location \u00b6 The MLCube root directory will contain project source files, resources required for training, other files to recreate run time (such as requirements.txt, docker and singularity recipes etc.). We need to copy two files: mnist.py that implements training and requirements.txt that lists python dependencies. By doing so, we are enforcing reproducibility. A developer of this MLCube wants to make it easier to run their training workload in a great variety of environments including universities, commercial companies, HPC-friendly organizations such as national labs. One way to achieve it is to use container runtime such as docker or singularity. So, we'll provide both docker file and singularity recipe that we'll put into the MLCube root directory as well. Thus, we'll make this directory a build context. For reasons that we will explain later, we also need to add .dockerignore file (that contains single line - workspace/ ). The MLCube directory now looks like: mnist/ .dockerignore Dockerfile mnist.py requirements.txt Singularity.recipe A good test at this point would be to ensure that project is runnable from the build directory, and docker and singularity images can be built. MLCube definition file \u00b6 At this point we are ready to create a cube definition file. This is the first definition file that makes a folder to be an MLCube folder. This is a YAML file that provides information such as name, author, version, named as mlcube.yaml and located in the cube root directory . The most important section is the one that lists what tasks are implemented in this cube: # Name of this MLCube. name : mnist # Brief description for this MLCube. description : MLCommons MNIST MLCube example # List of authors/developers. authors : - name : \"First Second\" email : \"first.second@company.com\" org : \"Company Inc.\" # Platform description. This is where users can specify MLCube resource requirements, such as # number of accelerators, memory and disk requirements etc. The exact structure and intended # usage of information in this section is work in progress. This section is optional now. platform : accelerator_count : 0 accelerator_maker : NVIDIA accelerator_model : A100-80GB host_memory_gb : 40 need_internet_access : True host_disk_space_gb : 100 # Configuration for docker runner (additional options can be configured in system settings file). docker : image : mlcommons/mnist:0.0.1 # Configuration for singularity runner (additional options can be configured in system settings # file). singularity : image : mnist-0.0.1.simg # Section where MLCube tasks are defined. tasks : # `Download` task. It has one input and two output parameters. download : parameters : inputs : { data_config : data.yaml } outputs : { data_dir : data/ , log_dir : logs/ } # `Train` task. It has two input and two output parameters. train : parameters : inputs : { data_dir : data/ , train_config : train.yaml } outputs : { log_dir : logs/ , model_dir : model/ } At this point, the directory looks like: mnist/ .dockerignore Dockerfile mlcube.yaml mnist.py requirements.txt Singularity.recipe Workspace \u00b6 The workspace is a directory inside cube ( workspace ) where, by default, input/output file system artifacts are stored. There are multiple reasons to have one. One is to formally have default place for data sets, configuration and log files etc. Having all these parameters in one place makes it simpler to run cubes on remote hosts and then sync results back to users' local machines. We need to be able to provide URI and hash of the MNIST dataset, collection of hyper-parameters and formally define a directory to store logs, models and MNIST data set. To do so, we create the directory tree workspace/ , and then create two files with the following content ( data.yaml ): uri : https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz hash : 731c5ac602752760c8e48fbffcf8c3b850d9dc2a2aedcf2cc48468fc17b673d1 and train.yaml : optimizer : \"adam\" train_epochs : 5 batch_size : 32 At this point, the cube directory looks like: mnist/ workspace/ data.yaml train.yaml .dockerignore Dockerfile mlcube.yaml mnist.py requirements.txt Singularity.recipe MNIST MLCube directory structure summary \u00b6 mnist/ workspace/ # Default location for data sets, logs, models, parameter files. data.yaml # URI and hash of MNIST dataset. train.yaml # Train hyper-parameters. .dockerignore # Docker ignore file that prevents workspace directory to be sent to docker server. Dockerfile # Docker recipe. mlcube.yaml # MLCube definition file. mnist.py # Python source code training simple neural network using MNIST data set. requirements.txt # Python project dependencies. Singularity.recipe # Singularity recipe. Running MNIST MLCube \u00b6 We need to set up the Python virtual environment. These are the steps outlined in the Introduction section except we do not clone GitHub repository with the example MLCube cubes. # Create Python Virtual Environment virtualenv -p python3.6 ./env && source ./env/bin/activate # Install MLCube Docker and Singularity runners pip install mlcube-docker mlcube-singularity Before running MNIST cube below, it is probably a good idea to remove tasks' outputs from previous runs that are located in the workspace directory. All directories except can be removed. Docker Runner \u00b6 Configure MNIST cube (this is optional step, docker runner checks if image exists, and if it does not, runs configure phase automatically): mlcube configure --mlcube=. --platform=docker Run two tasks - download (download data) and train (train tiny neural network): mlcube run --mlcube=. --platform=docker --task=download mlcube run --mlcube=. --platform=docker --task=train Singularity Runner \u00b6 Configure MNIST cube: mlcube configure --mlcube=. --platform=singularity Run two tasks - download (download data) and train (train tiny neural network): mlcube run --mlcube=. --platform=singularity --task=download mlcube run --mlcube=. --platform=singularity --task=train","title":"MNIST"},{"location":"getting-started/mnist/#mnist","text":"The MNIST dataset is a collection of 60,000 handwritten digits widely used for training statistical, Machine Learning (ML) and Deep Learning (DL) models. The MNIST MLCube example demonstrates how data scientists, ML and DL researchers and developers can distribute their ML projects (including training, validation and inference code) as MLCube cubes. MLCube establishes a standard to package user workloads, and provides unified command line interface. In addition, MLCube provides a number of reference runners - python packages that can run cubes on different platforms including Docker , Singularity , KubeFlow and several others. A data scientist has been working on a machine learning project. The goal is to train a simple neural network to classify the collection of 60,000 small images into 10 classes. The source files for this MNIST example can be found on GitHub in MLCube Example repository .","title":"MNIST"},{"location":"getting-started/mnist/#mnist-training-code","text":"Training an ML model is a process involving multiple steps such as downloading data, analyzing and cleaning data, splitting data into train/validation/test data sets, running hyper-parameter optimization experiments and performing final model testing. MNIST dataset is a relatively small and well studied dataset that provides standard train/test split. In this simple example a developer needs to implement two steps - (1) downloading data and (2) training a model. We call these steps as tasks . Each task requires several parameters, such as a URL of the data set that we need to download, location on a local disk where the data set will be downloaded to, path to a directory that will contain training artifacts such as log files, training snapshots and Machine Learning models. We can characterize these two tasks in the following way: Download task: Inputs : A yaml file ( data_config ) with two parameters - dataset URI and dataset hash. Outputs : Directory to serialize the data set ( data_dir ) and directory to serialize log files ( log_dir ). Training task: Inputs : Directory with MNIST data set ( data_dir ), training hyper-parameters defined in a file ( train_config ). Outputs : Directory to store training results ( model_dir ) and directory to store log files ( log_dir ). We have intentionally made all input/output parameters to be file system artifacts. By doing so, we support reproducibility. Instead of command line arguments that can easily be lost, we store them in files. There are many ways to implement the MNIST example. For simplicity, we assume the following: We use one python file. Task name (download, train) is a command line positional parameter. Both tasks write logs, so it makes sense to add a parameter that defines a directory for log files. The download task accepts additional data directory parameter. The train task accepts such parameters as data and model directories, path to a file with hyper-parameter. Configurable hyper-parameters are: (1) optimizer name, (2) number of training epochs and (3) global batch size. Then, our implementation could look like this. Parse command line arguments and identify a task to run. If it is the download task, call a function that downloads data sets. If it is the train task, train a model. This is sort of single entrypoint implementation where we run one script asking to perform various tasks. We run our script (mnist.py) in the following way: python mnist.py download --data_config=PATH --data_dir=PATH --log_dir=PATH python mnist.py train --train_config=PATH --data_dir=PATH --model_dir=PATH --log_dir=PATH","title":"MNIST training code"},{"location":"getting-started/mnist/#mlcube-implementation","text":"Packaging our MNIST training script as a MLCube is done in several steps. We will be using a directory-based cube where a directory is structured in a certain way and contains specific files that make it MLCube compliant. We need to create an empty directory on a local disk. Let's assume we call it mnist and we'll use {MLCUBE_ROOT} to denote a full path to this directory. This is called an MLCube root directory. At this point this directory is empty: mnist/","title":"MLCube implementation"},{"location":"getting-started/mnist/#build-location","text":"The MLCube root directory will contain project source files, resources required for training, other files to recreate run time (such as requirements.txt, docker and singularity recipes etc.). We need to copy two files: mnist.py that implements training and requirements.txt that lists python dependencies. By doing so, we are enforcing reproducibility. A developer of this MLCube wants to make it easier to run their training workload in a great variety of environments including universities, commercial companies, HPC-friendly organizations such as national labs. One way to achieve it is to use container runtime such as docker or singularity. So, we'll provide both docker file and singularity recipe that we'll put into the MLCube root directory as well. Thus, we'll make this directory a build context. For reasons that we will explain later, we also need to add .dockerignore file (that contains single line - workspace/ ). The MLCube directory now looks like: mnist/ .dockerignore Dockerfile mnist.py requirements.txt Singularity.recipe A good test at this point would be to ensure that project is runnable from the build directory, and docker and singularity images can be built.","title":"Build location"},{"location":"getting-started/mnist/#mlcube-definition-file","text":"At this point we are ready to create a cube definition file. This is the first definition file that makes a folder to be an MLCube folder. This is a YAML file that provides information such as name, author, version, named as mlcube.yaml and located in the cube root directory . The most important section is the one that lists what tasks are implemented in this cube: # Name of this MLCube. name : mnist # Brief description for this MLCube. description : MLCommons MNIST MLCube example # List of authors/developers. authors : - name : \"First Second\" email : \"first.second@company.com\" org : \"Company Inc.\" # Platform description. This is where users can specify MLCube resource requirements, such as # number of accelerators, memory and disk requirements etc. The exact structure and intended # usage of information in this section is work in progress. This section is optional now. platform : accelerator_count : 0 accelerator_maker : NVIDIA accelerator_model : A100-80GB host_memory_gb : 40 need_internet_access : True host_disk_space_gb : 100 # Configuration for docker runner (additional options can be configured in system settings file). docker : image : mlcommons/mnist:0.0.1 # Configuration for singularity runner (additional options can be configured in system settings # file). singularity : image : mnist-0.0.1.simg # Section where MLCube tasks are defined. tasks : # `Download` task. It has one input and two output parameters. download : parameters : inputs : { data_config : data.yaml } outputs : { data_dir : data/ , log_dir : logs/ } # `Train` task. It has two input and two output parameters. train : parameters : inputs : { data_dir : data/ , train_config : train.yaml } outputs : { log_dir : logs/ , model_dir : model/ } At this point, the directory looks like: mnist/ .dockerignore Dockerfile mlcube.yaml mnist.py requirements.txt Singularity.recipe","title":"MLCube definition file"},{"location":"getting-started/mnist/#workspace","text":"The workspace is a directory inside cube ( workspace ) where, by default, input/output file system artifacts are stored. There are multiple reasons to have one. One is to formally have default place for data sets, configuration and log files etc. Having all these parameters in one place makes it simpler to run cubes on remote hosts and then sync results back to users' local machines. We need to be able to provide URI and hash of the MNIST dataset, collection of hyper-parameters and formally define a directory to store logs, models and MNIST data set. To do so, we create the directory tree workspace/ , and then create two files with the following content ( data.yaml ): uri : https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz hash : 731c5ac602752760c8e48fbffcf8c3b850d9dc2a2aedcf2cc48468fc17b673d1 and train.yaml : optimizer : \"adam\" train_epochs : 5 batch_size : 32 At this point, the cube directory looks like: mnist/ workspace/ data.yaml train.yaml .dockerignore Dockerfile mlcube.yaml mnist.py requirements.txt Singularity.recipe","title":"Workspace"},{"location":"getting-started/mnist/#mnist-mlcube-directory-structure-summary","text":"mnist/ workspace/ # Default location for data sets, logs, models, parameter files. data.yaml # URI and hash of MNIST dataset. train.yaml # Train hyper-parameters. .dockerignore # Docker ignore file that prevents workspace directory to be sent to docker server. Dockerfile # Docker recipe. mlcube.yaml # MLCube definition file. mnist.py # Python source code training simple neural network using MNIST data set. requirements.txt # Python project dependencies. Singularity.recipe # Singularity recipe.","title":"MNIST MLCube directory structure summary"},{"location":"getting-started/mnist/#running-mnist-mlcube","text":"We need to set up the Python virtual environment. These are the steps outlined in the Introduction section except we do not clone GitHub repository with the example MLCube cubes. # Create Python Virtual Environment virtualenv -p python3.6 ./env && source ./env/bin/activate # Install MLCube Docker and Singularity runners pip install mlcube-docker mlcube-singularity Before running MNIST cube below, it is probably a good idea to remove tasks' outputs from previous runs that are located in the workspace directory. All directories except can be removed.","title":"Running MNIST MLCube"},{"location":"getting-started/mnist/#docker-runner","text":"Configure MNIST cube (this is optional step, docker runner checks if image exists, and if it does not, runs configure phase automatically): mlcube configure --mlcube=. --platform=docker Run two tasks - download (download data) and train (train tiny neural network): mlcube run --mlcube=. --platform=docker --task=download mlcube run --mlcube=. --platform=docker --task=train","title":"Docker Runner"},{"location":"getting-started/mnist/#singularity-runner","text":"Configure MNIST cube: mlcube configure --mlcube=. --platform=singularity Run two tasks - download (download data) and train (train tiny neural network): mlcube run --mlcube=. --platform=singularity --task=download mlcube run --mlcube=. --platform=singularity --task=train","title":"Singularity Runner"},{"location":"runners/","text":"Runners \u00b6 MLCube runners run MLCube cubes on one or multiple platforms. Examples of platforms are Docker and Singularity containers, Kubernetes, remote hosts, virtual machines in the cloud, etc. Every runner has a fixed set of configuration parameters that users can change to configure MLCubes and runners for their environments. Concretely, runners can take information from three different sources: - MLCube configuration files that are located in the root directory of each file-system based MLCube. Parameters in these files configure generic parameters common for all environments, such as for instance, docker image names. - MLCube system settings file that is located (by default) in the user home directory ( ~/mlcube.yaml ). This file is created automatically, and can be used to configure parameters common for all MLCubes in a particular environments. They can include docker executable, GPU and CPU docker arguments, user SSH and cloud credentials, etc. - Optionally, runners can use parameters defined in platform section of MLCube configuration file. This section usually contains information about such requirements as memory and persistent storage requirements, number of accelerators etc. MLCube standard requires that all runners implement mandatory functionality. All reference runners implement it. Users can develop their own runners to meet their specific requirements, such as security, authentication and authorization policies, and others. Reference MLCube runners \u00b6 Reference runners are: Docker Runner : Runs cubes locally using docker runtime. GCP Runner : Runs cubes in Google cloud. Kubernetes Runner : Runs cubes in Kubernetes. Kubeflow Runner : Runs cubes using Kubeflow. Singularity Runner : Runs cubes using singularity runtime. SSH Runner : Runs cubes on remote hosts. SSH Runner uses other runners, such as Docker or Singularity runners, to run cubes on remote hosts. Runner commands \u00b6 Each runner exposes mandatory and optional functionality through a set of commands. This is similar to, for instance, how Git implements its CLI ( git followed by a specific command such as checkout , pull , push etc.). Mandatory MLCube runner commands are configure and run : configure : Configure MLCube. Exact functionality depends on a runner type, but the goal is to ensure that a cube is ready to run. The following are the examples of what can be done at configure phase: build docker or singularity container, create python virtual environment, allocate and configure virtual machine in the cloud, copy cube to a remote host etc. Once configuration is successfully completed, it is assumed a runner can run that cube. run : Run tasks defined in MLCube. Reference runners recognize three parameters - mlcube, platform and task. mlcube : Path to a cube root directory. In future versions, this can be a URI with a specific protocol. Runners could support various MLCube implementations (excluding reference directory-based) such as docker/singularity containers, GitHub repositories, compressed archives and others. platform : Name of a platform. By default, runners create standard platform configurations in MLCube system settings file with predefined names. Users can change those names and use them on a command line. For instance, they can have different names for an 8-way GPU server and a simple CPU-based server for SSH runner. task : Name of a task, or comma-separated list of tasks. Command line interface \u00b6 One way to run a MLCube is to follow the following template supported by all reference runners: mlcube COMMAND --mlcube = MLCUBE_ROOT_DIRECTORY --platform = PLATFORM_NAME --task = TASK_NAME Example command to configure MNIST Docker-based MLCube: mlcube configure --mlcube = examples/mnist --platform = docker Example command to run two tasks implemented by the MNIST Docker-based MLCube: mlcube run --mlcube = examples/mnist --platform = docker --task = download mlcube run --mlcube = examples/mnist --platform = docker --task = train Configuration subsystem \u00b6 Runners are configured using information from three different sources: The base configuration comes from the system settings file. By default, the location of this file is ${HOME}/mlflow.yaml . It is created automatically whenever a user runs mlcube command line tool. The purpose of this file is to provide system-wide configuration for runners that are specific to user and their environment. This is kind of information that should not generally present in MLCube configuration files (next item). It shoud include such information as docker executable (docker, sudo docker, nvidia-docker, podman, etc), docker-specific runtime arguments, user credentials for GCP and remote hosts, information about remote hosts etc. The MLCube configuration file that is available with each MLCube cube. This file contains (as of now) such parameters, as docker and singularity image names, MLCube resource requirements and tasks. This information overrides information from system settings file. Configuration that is provided on a command line. Users are allowed (but not encouraged) to override parameters on the fly when they run MLCube cubes. MLCube System settings file \u00b6 Example of MLCube system settings file ( ${HOME}/mlcube.yaml ) is the following. As it was mentioned above, it is created automatically by searching packages that start with mlcube_ . Such packages must provide get_runner_class function that must return a runner class derived from Runner . # This section maps a runner name to a runner package. This is one way how developers can plug in # their custom runners. Python package, or this type of association, could be one of many ways to # implement runners. runners : docker : # MLCube Docker reference runner pkg : mlcube_docker gcp : # MLCube Google Cloud Platform reference runner pkg : mlcube_gcp k8s : # MLCube Kubernetes reference runner pkg : mlcube_k8s kubeflow : # MLCube KubeFlow reference runner pkg : mlcube_kubeflow singularity : # MLCube Singularity reference runner pkg : mlcube_singularity ssh : # MLCube SSH reference runner pkg : mlcube_ssh # This section defines configurations for the above runners. It is a dictionary mapping platform # name to a runner configuration. These names could be any names. For instance, users can have # two platforms for an SSH runner pointing to two different remote hosts. The platform names are # those passed to mlcube tool using `--platform` command line argument. platforms : # Docker runner configuration. The only parameter that is supposed to be present in MLCube # configuration files is image name (`image`). For other parameters, see Docker Runner # documentation page. docker : runner : docker image : ${docker.image} docker : docker env_args : {} gpu_args : '' cpu_args : '' build_args : {} build_context : . build_file : Dockerfile build_strategy : pull # Google Cloud Platform runner. None of these configuration parameters are supposed to be # present in MLCube configuration files. For other parameters, see GCP Runner documentation # page. gcp : runner : gcp gcp : project_id : '' zone : '' credentials : '' instance : name : '' machine_type : '' disk_size_gb : '' platform : '' # Kubernetes runner. None of these configuration parameters are supposed to be present in # MLCube configuration files. For other parameters, see Kubernetes Runner documentation page. k8s : runner : k8s pvc : ${name} image : ${docker.image} namespace : default # Kubeflow runner. None of these configuration parameters are supposed to be present in # MLCube configuration files. For other parameters, see Kubeflow Runner documentation page. kubeflow : runner : kubeflow image : ${docker.image} pvc : ??? namespace : default pipeline_host : '' # Singularity runner configuration. The only parameter that is supposed to be present in MLCube # configuration files is image name (`image`). For other parameters, see Singularity Runner # documentation page. singularity : runner : singularity image : ${singularity.image} image_dir : ${runtime.workspace}/.image singularity : singularity build_args : --fakeroot build_file : Singularity.recipe # SSH runner. None of these configuration parameters are supposed to be present in # MLCube configuration files. For other parameters, see SSH Runner documentation page. ssh : runner : ssh host : '' platform : '' remote_root : '' interpreter : {} authentication : {} # Dedicated section to define future data `storage` layer. It's work in progress. storage : {} Users can and should update configuration parameters according to their environment. Also, please backup this file regularly. One possibility is to move this file to a location that is regularly snapshoted. When non-standard path is used, users must define a MLCUBE_SYSTEM_SETTINGS environment variable that points to this new location. Users can also duplicate runner sections assigning names accordingly, like it was mentioned above. For instance, users can have two ssh sections one for each different host: platforms : my_dev_server_1 : runner : ssh # Other parameters ... my_dev_server_2 : runner : ssh # Other parameters ... and then mlcube run --mlcube = . --task = MY_TASK --platform = my_dev_server_2 MLCube runtime provides minimal functionality to interact with system settings file: # Print system settings file mlcube config --list # Query a value associated with the particular key mlcube config --get runners mlcube config --get platforms.docker # Create a new fresh platform for this runner mlcube config --create-platform ssh my_dev_server_1 mlcube config --get platforms.my_dev_server_1 # Rename platform mlcube config --rename-platform my_dev_server_1 my_dev_server_2 mlcube config --get platforms.my_dev_server_2 # Remove platform from the system settings file mlcube config --remove-platform my_dev_server_2 # Create a new platform copying configuration of one of existing platforms. mlcube config --copy-platform EXISTING_PLATFORM NEW_PLATFORM # Rename existing runner mlcube config --rename-runner OLD_NAME NEW_NAME # Remove runner mlcube config --remove-runner NAME Removed standard runners (MLCube reference runners) will be recreated when mlcube runs next time.","title":"Runners"},{"location":"runners/#runners","text":"MLCube runners run MLCube cubes on one or multiple platforms. Examples of platforms are Docker and Singularity containers, Kubernetes, remote hosts, virtual machines in the cloud, etc. Every runner has a fixed set of configuration parameters that users can change to configure MLCubes and runners for their environments. Concretely, runners can take information from three different sources: - MLCube configuration files that are located in the root directory of each file-system based MLCube. Parameters in these files configure generic parameters common for all environments, such as for instance, docker image names. - MLCube system settings file that is located (by default) in the user home directory ( ~/mlcube.yaml ). This file is created automatically, and can be used to configure parameters common for all MLCubes in a particular environments. They can include docker executable, GPU and CPU docker arguments, user SSH and cloud credentials, etc. - Optionally, runners can use parameters defined in platform section of MLCube configuration file. This section usually contains information about such requirements as memory and persistent storage requirements, number of accelerators etc. MLCube standard requires that all runners implement mandatory functionality. All reference runners implement it. Users can develop their own runners to meet their specific requirements, such as security, authentication and authorization policies, and others.","title":"Runners"},{"location":"runners/#reference-mlcube-runners","text":"Reference runners are: Docker Runner : Runs cubes locally using docker runtime. GCP Runner : Runs cubes in Google cloud. Kubernetes Runner : Runs cubes in Kubernetes. Kubeflow Runner : Runs cubes using Kubeflow. Singularity Runner : Runs cubes using singularity runtime. SSH Runner : Runs cubes on remote hosts. SSH Runner uses other runners, such as Docker or Singularity runners, to run cubes on remote hosts.","title":"Reference MLCube runners"},{"location":"runners/#runner-commands","text":"Each runner exposes mandatory and optional functionality through a set of commands. This is similar to, for instance, how Git implements its CLI ( git followed by a specific command such as checkout , pull , push etc.). Mandatory MLCube runner commands are configure and run : configure : Configure MLCube. Exact functionality depends on a runner type, but the goal is to ensure that a cube is ready to run. The following are the examples of what can be done at configure phase: build docker or singularity container, create python virtual environment, allocate and configure virtual machine in the cloud, copy cube to a remote host etc. Once configuration is successfully completed, it is assumed a runner can run that cube. run : Run tasks defined in MLCube. Reference runners recognize three parameters - mlcube, platform and task. mlcube : Path to a cube root directory. In future versions, this can be a URI with a specific protocol. Runners could support various MLCube implementations (excluding reference directory-based) such as docker/singularity containers, GitHub repositories, compressed archives and others. platform : Name of a platform. By default, runners create standard platform configurations in MLCube system settings file with predefined names. Users can change those names and use them on a command line. For instance, they can have different names for an 8-way GPU server and a simple CPU-based server for SSH runner. task : Name of a task, or comma-separated list of tasks.","title":"Runner commands"},{"location":"runners/#command-line-interface","text":"One way to run a MLCube is to follow the following template supported by all reference runners: mlcube COMMAND --mlcube = MLCUBE_ROOT_DIRECTORY --platform = PLATFORM_NAME --task = TASK_NAME Example command to configure MNIST Docker-based MLCube: mlcube configure --mlcube = examples/mnist --platform = docker Example command to run two tasks implemented by the MNIST Docker-based MLCube: mlcube run --mlcube = examples/mnist --platform = docker --task = download mlcube run --mlcube = examples/mnist --platform = docker --task = train","title":"Command line interface"},{"location":"runners/#configuration-subsystem","text":"Runners are configured using information from three different sources: The base configuration comes from the system settings file. By default, the location of this file is ${HOME}/mlflow.yaml . It is created automatically whenever a user runs mlcube command line tool. The purpose of this file is to provide system-wide configuration for runners that are specific to user and their environment. This is kind of information that should not generally present in MLCube configuration files (next item). It shoud include such information as docker executable (docker, sudo docker, nvidia-docker, podman, etc), docker-specific runtime arguments, user credentials for GCP and remote hosts, information about remote hosts etc. The MLCube configuration file that is available with each MLCube cube. This file contains (as of now) such parameters, as docker and singularity image names, MLCube resource requirements and tasks. This information overrides information from system settings file. Configuration that is provided on a command line. Users are allowed (but not encouraged) to override parameters on the fly when they run MLCube cubes.","title":"Configuration subsystem"},{"location":"runners/#mlcube-system-settings-file","text":"Example of MLCube system settings file ( ${HOME}/mlcube.yaml ) is the following. As it was mentioned above, it is created automatically by searching packages that start with mlcube_ . Such packages must provide get_runner_class function that must return a runner class derived from Runner . # This section maps a runner name to a runner package. This is one way how developers can plug in # their custom runners. Python package, or this type of association, could be one of many ways to # implement runners. runners : docker : # MLCube Docker reference runner pkg : mlcube_docker gcp : # MLCube Google Cloud Platform reference runner pkg : mlcube_gcp k8s : # MLCube Kubernetes reference runner pkg : mlcube_k8s kubeflow : # MLCube KubeFlow reference runner pkg : mlcube_kubeflow singularity : # MLCube Singularity reference runner pkg : mlcube_singularity ssh : # MLCube SSH reference runner pkg : mlcube_ssh # This section defines configurations for the above runners. It is a dictionary mapping platform # name to a runner configuration. These names could be any names. For instance, users can have # two platforms for an SSH runner pointing to two different remote hosts. The platform names are # those passed to mlcube tool using `--platform` command line argument. platforms : # Docker runner configuration. The only parameter that is supposed to be present in MLCube # configuration files is image name (`image`). For other parameters, see Docker Runner # documentation page. docker : runner : docker image : ${docker.image} docker : docker env_args : {} gpu_args : '' cpu_args : '' build_args : {} build_context : . build_file : Dockerfile build_strategy : pull # Google Cloud Platform runner. None of these configuration parameters are supposed to be # present in MLCube configuration files. For other parameters, see GCP Runner documentation # page. gcp : runner : gcp gcp : project_id : '' zone : '' credentials : '' instance : name : '' machine_type : '' disk_size_gb : '' platform : '' # Kubernetes runner. None of these configuration parameters are supposed to be present in # MLCube configuration files. For other parameters, see Kubernetes Runner documentation page. k8s : runner : k8s pvc : ${name} image : ${docker.image} namespace : default # Kubeflow runner. None of these configuration parameters are supposed to be present in # MLCube configuration files. For other parameters, see Kubeflow Runner documentation page. kubeflow : runner : kubeflow image : ${docker.image} pvc : ??? namespace : default pipeline_host : '' # Singularity runner configuration. The only parameter that is supposed to be present in MLCube # configuration files is image name (`image`). For other parameters, see Singularity Runner # documentation page. singularity : runner : singularity image : ${singularity.image} image_dir : ${runtime.workspace}/.image singularity : singularity build_args : --fakeroot build_file : Singularity.recipe # SSH runner. None of these configuration parameters are supposed to be present in # MLCube configuration files. For other parameters, see SSH Runner documentation page. ssh : runner : ssh host : '' platform : '' remote_root : '' interpreter : {} authentication : {} # Dedicated section to define future data `storage` layer. It's work in progress. storage : {} Users can and should update configuration parameters according to their environment. Also, please backup this file regularly. One possibility is to move this file to a location that is regularly snapshoted. When non-standard path is used, users must define a MLCUBE_SYSTEM_SETTINGS environment variable that points to this new location. Users can also duplicate runner sections assigning names accordingly, like it was mentioned above. For instance, users can have two ssh sections one for each different host: platforms : my_dev_server_1 : runner : ssh # Other parameters ... my_dev_server_2 : runner : ssh # Other parameters ... and then mlcube run --mlcube = . --task = MY_TASK --platform = my_dev_server_2 MLCube runtime provides minimal functionality to interact with system settings file: # Print system settings file mlcube config --list # Query a value associated with the particular key mlcube config --get runners mlcube config --get platforms.docker # Create a new fresh platform for this runner mlcube config --create-platform ssh my_dev_server_1 mlcube config --get platforms.my_dev_server_1 # Rename platform mlcube config --rename-platform my_dev_server_1 my_dev_server_2 mlcube config --get platforms.my_dev_server_2 # Remove platform from the system settings file mlcube config --remove-platform my_dev_server_2 # Create a new platform copying configuration of one of existing platforms. mlcube config --copy-platform EXISTING_PLATFORM NEW_PLATFORM # Rename existing runner mlcube config --rename-runner OLD_NAME NEW_NAME # Remove runner mlcube config --remove-runner NAME Removed standard runners (MLCube reference runners) will be recreated when mlcube runs next time.","title":"MLCube System settings file"},{"location":"runners/docker-runner/","text":"Docker Runner \u00b6 Docker runner uses docker/nvidia-docker/podman to run MLCube cubes. It supports two mandatory commands - configure and run with standard arguments - mlcube , platform and task . Users can configure docker runner in MLCube configuration file, system setting file, and override parameters on a command line. Configuration parameters \u00b6 MLCube reference docker runner supports the following configuration parameters (with default values): # Docker Image name, for instance \"mlcommons/mnist:0.0.1\" image : ${docker.image} # Docker executable (docker, podman, sudo docker ...). docker : docker # Environmental variables for run command (-e name=value). env_args : {} # Docker run arguments when ${platform.accelerator_count} > 0. gpu_args : '' # Docker run arguments when ${platform.accelerator_count} == 0. cpu_args : '' # Docker build arguments (--build-arg name=value) build_args : {} # Docker build context relative to $MLCUBE_ROOT. Default is $MLCUBE_ROOT. build_context : . # Docker file relative to $MLCUBE_ROOT, default is `$MLCUBE_ROOT/Dockerfile`. build_file : Dockerfile # MLCube configuration strategy # 'pull': never try to build, always pull # 'auto': build if image not found and dockerfile found # 'always': build even if image found build_strategy : pull Configuring MLCubes \u00b6 Docker runner uses build_strategy configuration parameter to decide on build strategy: pull : always try to pull docker image, never attempt to build. auto : use build_context and build_file to decide if Dockerfile exists. If it exists, build the image. always : build docker image always when running MLCube tasks. Docker runner under the hood runs the following command line: ${docker.docker} build ${docker.build_args} -t ${docker.image} -f ${recipe} ${context} where: ${docker.docker} is the docker executable. ${docker.build_args} docker build arguments. ${docker.image} is the docker image name. ${recipe} is the ${docker.build_file} relative to context ${context} is the ${docker.build_context} relative to MLCube root directory. Users do not need to run the configure command explicitly, docker runner uses the following logic to decide what to do before running any task. If strategy is always , build the docker image. Else, if docker image exists, do nothing, else build or pull depending on what strategy is and if Dockerfile exists in MLCube directory. Running MLCubes \u00b6 Docker runner runs the following command: ${docker.docker} run {run_args} ${docker.env_args} {volumes} ${docker.image} {task_args} where: ${docker.docker} is the docker executable. {run_args} are either ${docker.cpu_args} or ${docker.gpu_args} depending on ${platform.num_accelerators} value. ${docker.env_args} are the docker environmental variables. {volumes} are the mount points that the runner automatically constructs based upon the task input/output specifications. ${docker.image} is the docker image name. {task_args} is the task command line arguments, constructed automatically by the runner.","title":"Docker Runner"},{"location":"runners/docker-runner/#docker-runner","text":"Docker runner uses docker/nvidia-docker/podman to run MLCube cubes. It supports two mandatory commands - configure and run with standard arguments - mlcube , platform and task . Users can configure docker runner in MLCube configuration file, system setting file, and override parameters on a command line.","title":"Docker Runner"},{"location":"runners/docker-runner/#configuration-parameters","text":"MLCube reference docker runner supports the following configuration parameters (with default values): # Docker Image name, for instance \"mlcommons/mnist:0.0.1\" image : ${docker.image} # Docker executable (docker, podman, sudo docker ...). docker : docker # Environmental variables for run command (-e name=value). env_args : {} # Docker run arguments when ${platform.accelerator_count} > 0. gpu_args : '' # Docker run arguments when ${platform.accelerator_count} == 0. cpu_args : '' # Docker build arguments (--build-arg name=value) build_args : {} # Docker build context relative to $MLCUBE_ROOT. Default is $MLCUBE_ROOT. build_context : . # Docker file relative to $MLCUBE_ROOT, default is `$MLCUBE_ROOT/Dockerfile`. build_file : Dockerfile # MLCube configuration strategy # 'pull': never try to build, always pull # 'auto': build if image not found and dockerfile found # 'always': build even if image found build_strategy : pull","title":"Configuration parameters"},{"location":"runners/docker-runner/#configuring-mlcubes","text":"Docker runner uses build_strategy configuration parameter to decide on build strategy: pull : always try to pull docker image, never attempt to build. auto : use build_context and build_file to decide if Dockerfile exists. If it exists, build the image. always : build docker image always when running MLCube tasks. Docker runner under the hood runs the following command line: ${docker.docker} build ${docker.build_args} -t ${docker.image} -f ${recipe} ${context} where: ${docker.docker} is the docker executable. ${docker.build_args} docker build arguments. ${docker.image} is the docker image name. ${recipe} is the ${docker.build_file} relative to context ${context} is the ${docker.build_context} relative to MLCube root directory. Users do not need to run the configure command explicitly, docker runner uses the following logic to decide what to do before running any task. If strategy is always , build the docker image. Else, if docker image exists, do nothing, else build or pull depending on what strategy is and if Dockerfile exists in MLCube directory.","title":"Configuring MLCubes"},{"location":"runners/docker-runner/#running-mlcubes","text":"Docker runner runs the following command: ${docker.docker} run {run_args} ${docker.env_args} {volumes} ${docker.image} {task_args} where: ${docker.docker} is the docker executable. {run_args} are either ${docker.cpu_args} or ${docker.gpu_args} depending on ${platform.num_accelerators} value. ${docker.env_args} are the docker environmental variables. {volumes} are the mount points that the runner automatically constructs based upon the task input/output specifications. ${docker.image} is the docker image name. {task_args} is the task command line arguments, constructed automatically by the runner.","title":"Running MLCubes"},{"location":"runners/gcp-runner/","text":"Google Compute Platform (GCP) Runner \u00b6 DISCLAIMER MLCube is under active development. Allocating and using instances in clouds are associated with costs. Users of GCP runners should be aware about it, especially, taking into account capability of GCP runners to automatically create and start remote instances. GCP RUNNERS in current implementation DO NOT stop/destroy remote instances. Users are encouraged to visit web consoles to identify what virtual instances exist and run. GCP runner can update users' ${HOME}/.ssh/config configuration files. GCP runner is a frontend runner for running MLCubes in Google cloud. It is called a frontend runner because it does not actually run cubes, but ensures that a remote instance is up and running, and then uses other runners to actually run MLCubes. The following chain of runners is supported and has been tested: 1. A user interacts with GCP runners. These runners are responsible for creating remote instances (if they do not exist), start them, install required software stack (such as docker or singularity). 2. Once a remote instance is up and running, GCP runners delegates further execution to other runners, such as SSH runners . SSH runners are responsible for delivering MLCubes to remote instances. SSH runners then delegate the actual execution of cubes on those remote instances to such runners as docker runner or singularity runner . The described scenario assumes the presence of the following platform configuration files: GCP, SSH and one of Docker or Singularity. As MLCube project evolves, other paths may become possible to run cubes in clouds such as GCP. Pre-requisites \u00b6 To use GCP runners, users need to have a GCP account. The following account details must be known and available in advance: 1. Project ID. 2. Zone. 3. Service account JSON file . 4. Users should configure their GCP accounts so that ever new virtual instance is automatically deployed with user public key making it available through SSH access automatically. Creating remote instances \u00b6 Remote instances for running MLCubes can be created manually or automatically. 1. To create a virtual instance manually, go to GCP console, select Compute Engine and then VM instances . Write down an instance name. 2. To create a virtual instance automatically, a GCP platform file needs be configured. A limited functionality is supported. Basically, users can only specify machine type and disk size . Ubuntu 18.04 OS will be used as a base image. Configuration parameters \u00b6 gcp : # These are your project ID and zone names. project_id : '' zone : '' # As described above, ensure you have service account activated and download your JSON key file. credentials : file : '${HOME}/.gcp/service_account_key_file.json' scopes : [ 'https://www.googleapis.com/auth/cloud-platform' ] # Instance parameters. # If existing remote instance is used, only `name` field is used. Other fields are not taken # into account. If users want GCP runners to automatically create remote instances, all three # fields must present. Instance name is arbitrary name for this instance. Machine type must be # the valid GCP machine type. Ubuntu 18.04 is used as a base OS. instance : name : '' machine_type : '' disk_size_gb : '' # As described above, primary role of GCP runners is to ensure a remote instance exists before # delegating the actual `MLCube run` functionality to other runners. Currently, the only available # option is an SSH runner (that assumes remote instances are available vis SSH i.e. they have # public IPs). The `platform` field below specifies what runner the GCP runner should be using # once GCP virtual instance has been created. An SSH runner needs to be configured separately # (see sections below for some recommendations and best practices). platform : '' Configuring MLCubes \u00b6 GCP runners execute the following steps during the configuration phase: 1. Check that SSH access has been configured. A runner loads users ${HOME}/.ssh/config file and verifies it contains a section for the remote instance there (specified by the name). The configuration section must define User and IdentityFile . 2. GCP runner connects to GCP using provided project ID, zone name and credentials (file name and scopes). 3. GCP runner checks if a remote instance exists with the provided name. If it does not exist, it creates it using three parameters described above - instance name, machine type and disk size. 4. If a remote instance is not running, GCP runner starts it. 5. GCP runner retrieves a remote instance's metadata that includes public IP address. If public IP address does not match HostName in ssh configuration file, GCP RUNNER UPDATES USER SSH CONFIG FILE . 6. Currently, GCP runner automatically installs such packages, as docker , python3 and virtualenv . 7. GCP runner calls SSH runner to continue configuring remote instance in a MLCube-specific way. Running MLCubes \u00b6 GCP runner does not implement any specific logic and redirects its functionality to an SSH runner. Recommendations \u00b6 One remote instance can be used to run different MLCubes. Names of remote instances can reflect their type, for instance, gcp_free_micro_instance , gcp_4_cpu_instance , gcp_1_gpu_instance , gcp_8_gpu_instance etc. Following the above guidelines, these instances must be configured with key-based SSH access (GCP and SSH runners depend on this). Each remote instance must have a section in the {HOME}/.ssh/config that should look like: Host mlcube-gcp-instance-n1s4 HostName {{PUBLIC_IP_ADDRESS}} IdentityFile ~/.ssh/gcp_rsa User {{GCP_USER_LOGIN_NAME}} GCP runner will update the HostName value if actual IP address differs from existing one. Other fields are never updated by GCP runners. Section like this one is sufficient to partially configure GCP and fully configure SSH runners. After every GCP run, decide if a remote instance needs to be stopped/destroyed. If so, go to web console.","title":"GCP Runner"},{"location":"runners/gcp-runner/#google-compute-platform-gcp-runner","text":"DISCLAIMER MLCube is under active development. Allocating and using instances in clouds are associated with costs. Users of GCP runners should be aware about it, especially, taking into account capability of GCP runners to automatically create and start remote instances. GCP RUNNERS in current implementation DO NOT stop/destroy remote instances. Users are encouraged to visit web consoles to identify what virtual instances exist and run. GCP runner can update users' ${HOME}/.ssh/config configuration files. GCP runner is a frontend runner for running MLCubes in Google cloud. It is called a frontend runner because it does not actually run cubes, but ensures that a remote instance is up and running, and then uses other runners to actually run MLCubes. The following chain of runners is supported and has been tested: 1. A user interacts with GCP runners. These runners are responsible for creating remote instances (if they do not exist), start them, install required software stack (such as docker or singularity). 2. Once a remote instance is up and running, GCP runners delegates further execution to other runners, such as SSH runners . SSH runners are responsible for delivering MLCubes to remote instances. SSH runners then delegate the actual execution of cubes on those remote instances to such runners as docker runner or singularity runner . The described scenario assumes the presence of the following platform configuration files: GCP, SSH and one of Docker or Singularity. As MLCube project evolves, other paths may become possible to run cubes in clouds such as GCP.","title":"Google Compute Platform (GCP) Runner"},{"location":"runners/gcp-runner/#pre-requisites","text":"To use GCP runners, users need to have a GCP account. The following account details must be known and available in advance: 1. Project ID. 2. Zone. 3. Service account JSON file . 4. Users should configure their GCP accounts so that ever new virtual instance is automatically deployed with user public key making it available through SSH access automatically.","title":"Pre-requisites"},{"location":"runners/gcp-runner/#creating-remote-instances","text":"Remote instances for running MLCubes can be created manually or automatically. 1. To create a virtual instance manually, go to GCP console, select Compute Engine and then VM instances . Write down an instance name. 2. To create a virtual instance automatically, a GCP platform file needs be configured. A limited functionality is supported. Basically, users can only specify machine type and disk size . Ubuntu 18.04 OS will be used as a base image.","title":"Creating remote instances"},{"location":"runners/gcp-runner/#configuration-parameters","text":"gcp : # These are your project ID and zone names. project_id : '' zone : '' # As described above, ensure you have service account activated and download your JSON key file. credentials : file : '${HOME}/.gcp/service_account_key_file.json' scopes : [ 'https://www.googleapis.com/auth/cloud-platform' ] # Instance parameters. # If existing remote instance is used, only `name` field is used. Other fields are not taken # into account. If users want GCP runners to automatically create remote instances, all three # fields must present. Instance name is arbitrary name for this instance. Machine type must be # the valid GCP machine type. Ubuntu 18.04 is used as a base OS. instance : name : '' machine_type : '' disk_size_gb : '' # As described above, primary role of GCP runners is to ensure a remote instance exists before # delegating the actual `MLCube run` functionality to other runners. Currently, the only available # option is an SSH runner (that assumes remote instances are available vis SSH i.e. they have # public IPs). The `platform` field below specifies what runner the GCP runner should be using # once GCP virtual instance has been created. An SSH runner needs to be configured separately # (see sections below for some recommendations and best practices). platform : ''","title":"Configuration parameters"},{"location":"runners/gcp-runner/#configuring-mlcubes","text":"GCP runners execute the following steps during the configuration phase: 1. Check that SSH access has been configured. A runner loads users ${HOME}/.ssh/config file and verifies it contains a section for the remote instance there (specified by the name). The configuration section must define User and IdentityFile . 2. GCP runner connects to GCP using provided project ID, zone name and credentials (file name and scopes). 3. GCP runner checks if a remote instance exists with the provided name. If it does not exist, it creates it using three parameters described above - instance name, machine type and disk size. 4. If a remote instance is not running, GCP runner starts it. 5. GCP runner retrieves a remote instance's metadata that includes public IP address. If public IP address does not match HostName in ssh configuration file, GCP RUNNER UPDATES USER SSH CONFIG FILE . 6. Currently, GCP runner automatically installs such packages, as docker , python3 and virtualenv . 7. GCP runner calls SSH runner to continue configuring remote instance in a MLCube-specific way.","title":"Configuring MLCubes"},{"location":"runners/gcp-runner/#running-mlcubes","text":"GCP runner does not implement any specific logic and redirects its functionality to an SSH runner.","title":"Running MLCubes"},{"location":"runners/gcp-runner/#recommendations","text":"One remote instance can be used to run different MLCubes. Names of remote instances can reflect their type, for instance, gcp_free_micro_instance , gcp_4_cpu_instance , gcp_1_gpu_instance , gcp_8_gpu_instance etc. Following the above guidelines, these instances must be configured with key-based SSH access (GCP and SSH runners depend on this). Each remote instance must have a section in the {HOME}/.ssh/config that should look like: Host mlcube-gcp-instance-n1s4 HostName {{PUBLIC_IP_ADDRESS}} IdentityFile ~/.ssh/gcp_rsa User {{GCP_USER_LOGIN_NAME}} GCP runner will update the HostName value if actual IP address differs from existing one. Other fields are never updated by GCP runners. Section like this one is sufficient to partially configure GCP and fully configure SSH runners. After every GCP run, decide if a remote instance needs to be stopped/destroyed. If so, go to web console.","title":"Recommendations"},{"location":"runners/kubeflow/","text":"Kubeflow Runner \u00b6 Kubeflow supports two mandatory commands - configure and run with standard arguments - mlcube , platform and task . Users can configure SSH runner in system setting file, and override parameters on a command line. The configure command is not required, and does nothing when invoked. Configuration parameters \u00b6 # Use image name from docker configuration section image : ${docker.image} # PVC must point to the active MLCube workspace now. pvc : '???' # eg: set http://127.0.0.1:8000/pipeline when port forwarded svc/ml-pipeline-ui to port 8000 pipeline_host : '' Configuring MLCubes \u00b6 This runner does not need configure step. Running MLCubes \u00b6 To be done.","title":"Kubeflow Runner"},{"location":"runners/kubeflow/#kubeflow-runner","text":"Kubeflow supports two mandatory commands - configure and run with standard arguments - mlcube , platform and task . Users can configure SSH runner in system setting file, and override parameters on a command line. The configure command is not required, and does nothing when invoked.","title":"Kubeflow Runner"},{"location":"runners/kubeflow/#configuration-parameters","text":"# Use image name from docker configuration section image : ${docker.image} # PVC must point to the active MLCube workspace now. pvc : '???' # eg: set http://127.0.0.1:8000/pipeline when port forwarded svc/ml-pipeline-ui to port 8000 pipeline_host : ''","title":"Configuration parameters"},{"location":"runners/kubeflow/#configuring-mlcubes","text":"This runner does not need configure step.","title":"Configuring MLCubes"},{"location":"runners/kubeflow/#running-mlcubes","text":"To be done.","title":"Running MLCubes"},{"location":"runners/kubernetes/","text":"Kubernetes Runner \u00b6 The Kubernetes Runner runs a MLCube on a Kubernetes cluster. Why Kubernetes? \u00b6 One of the key goals of the MLCube project is to enable portability of ML models. Kubernetes offers a good set of abstractions to enable model training to be portable across different compute platforms. Design \u00b6 Kubernetes Runner Proposal Doc The Kubernetes runner takes in a kubernetes specific task file in the run directory and re-uses the Docker runner platform config and prepares a Kubernetes Job manifest. The runner then creates the job on the Kubernetes cluster. Configuration parameters \u00b6 Currently, users must create persistent volume claim (PVC) that points to an actual MLCube workspace directory. # By default, PVC name equals to the name of this MLCube (mnist, matmul, ...). pvc : ${name} # Use image name from docker configuration section. image : ${docker.image} The Kubernetes runner constructs the following Kubernetes Job manifest. apiVersion : batch/v1 kind : Job metadata : namespace : default generateName : mlcube-mnist- spec : template : spec : containers : - name : mlcube-container image : mlcommons/mlcube:mnist args : - --data_dir=/mnt/mlcube/mlcube-input/workspace/data - --model_dir=/mnt/mlcube/mlcube-output/workspace/model volumeMounts : - name : mlcube-input mountPath : /mnt/mlcube/mlcube-input - name : mlcube-output mountPath : /mnt/mlcube/mlcube-output volumes : - name : mlcube-input persistentVolumeClaim : claimName : mlcube-input - name : mlcube-output persistentVolumeClaim : claimName : mlcube-output restartPolicy : Never backoffLimit : 4 Configuring MLCubes \u00b6 This runner does not need configure step. Running MLCubes \u00b6 Algorithm is following: Load Kubernetes configuration. Create job manifest (see above). Create job and wait for completion.","title":"Kubernetes Runner"},{"location":"runners/kubernetes/#kubernetes-runner","text":"The Kubernetes Runner runs a MLCube on a Kubernetes cluster.","title":"Kubernetes Runner"},{"location":"runners/kubernetes/#why-kubernetes","text":"One of the key goals of the MLCube project is to enable portability of ML models. Kubernetes offers a good set of abstractions to enable model training to be portable across different compute platforms.","title":"Why Kubernetes?"},{"location":"runners/kubernetes/#design","text":"Kubernetes Runner Proposal Doc The Kubernetes runner takes in a kubernetes specific task file in the run directory and re-uses the Docker runner platform config and prepares a Kubernetes Job manifest. The runner then creates the job on the Kubernetes cluster.","title":"Design"},{"location":"runners/kubernetes/#configuration-parameters","text":"Currently, users must create persistent volume claim (PVC) that points to an actual MLCube workspace directory. # By default, PVC name equals to the name of this MLCube (mnist, matmul, ...). pvc : ${name} # Use image name from docker configuration section. image : ${docker.image} The Kubernetes runner constructs the following Kubernetes Job manifest. apiVersion : batch/v1 kind : Job metadata : namespace : default generateName : mlcube-mnist- spec : template : spec : containers : - name : mlcube-container image : mlcommons/mlcube:mnist args : - --data_dir=/mnt/mlcube/mlcube-input/workspace/data - --model_dir=/mnt/mlcube/mlcube-output/workspace/model volumeMounts : - name : mlcube-input mountPath : /mnt/mlcube/mlcube-input - name : mlcube-output mountPath : /mnt/mlcube/mlcube-output volumes : - name : mlcube-input persistentVolumeClaim : claimName : mlcube-input - name : mlcube-output persistentVolumeClaim : claimName : mlcube-output restartPolicy : Never backoffLimit : 4","title":"Configuration parameters"},{"location":"runners/kubernetes/#configuring-mlcubes","text":"This runner does not need configure step.","title":"Configuring MLCubes"},{"location":"runners/kubernetes/#running-mlcubes","text":"Algorithm is following: Load Kubernetes configuration. Create job manifest (see above). Create job and wait for completion.","title":"Running MLCubes"},{"location":"runners/singularity-runner/","text":"Singularity Runner \u00b6 Singularity runner uses singularity to run MLCube cubes. It supports two mandatory commands - configure and run with standard arguments - mlcube , platform and task . Users can configure Singularity runner in MLCube configuration file, system setting file, and override parameters on a command line. Configuration parameters \u00b6 MLCube reference singularity runner supports the following configuration parameters (with default values): # Name of a singularity image, for instance \"mnist-0.0.1.simg\". image : ${singularity.image} # Path where to build the image. By default, it is `.image` inside workspace directory. image_dir : ${runtime.workspace}/.image # Singularity executable singularity : singularity # Build arguments build_args : --fakeroot # Singularity recipe file relative to workspace. build_file : Singularity.recipe Configuring MLCubes \u00b6 Users do not need to run the configure command manually, singularity docker runs this whenever image is not found. Singularity runner under the hood runs the following command line: cd {recipe_path} && ${singularity} build ${build_args} {image_uri} ${build_file} where: {recipe_path} is the MLCube root directory. ${singularity} is the singularity executable. ${build_args} is the singularity build arguments. {image_uri} is the full image path ( ${image_dir}/${image} ). ${build_file} is the singularity build file. Running MLCubes \u00b6 Singularity runner runs the following command: ${singularity} run {volumes} {image_path} {task_args} where: ${singularity} is the singularity executable. {volumes} are the mount points that the runner automatically constructs based upon the task input/output specifications. {image_path} is the path to Singularity image ( {image_dir}/{image} ). {task_args} is the task command line arguments, constructed automatically by the runner.","title":"Singularity Runner"},{"location":"runners/singularity-runner/#singularity-runner","text":"Singularity runner uses singularity to run MLCube cubes. It supports two mandatory commands - configure and run with standard arguments - mlcube , platform and task . Users can configure Singularity runner in MLCube configuration file, system setting file, and override parameters on a command line.","title":"Singularity Runner"},{"location":"runners/singularity-runner/#configuration-parameters","text":"MLCube reference singularity runner supports the following configuration parameters (with default values): # Name of a singularity image, for instance \"mnist-0.0.1.simg\". image : ${singularity.image} # Path where to build the image. By default, it is `.image` inside workspace directory. image_dir : ${runtime.workspace}/.image # Singularity executable singularity : singularity # Build arguments build_args : --fakeroot # Singularity recipe file relative to workspace. build_file : Singularity.recipe","title":"Configuration parameters"},{"location":"runners/singularity-runner/#configuring-mlcubes","text":"Users do not need to run the configure command manually, singularity docker runs this whenever image is not found. Singularity runner under the hood runs the following command line: cd {recipe_path} && ${singularity} build ${build_args} {image_uri} ${build_file} where: {recipe_path} is the MLCube root directory. ${singularity} is the singularity executable. ${build_args} is the singularity build arguments. {image_uri} is the full image path ( ${image_dir}/${image} ). ${build_file} is the singularity build file.","title":"Configuring MLCubes"},{"location":"runners/singularity-runner/#running-mlcubes","text":"Singularity runner runs the following command: ${singularity} run {volumes} {image_path} {task_args} where: ${singularity} is the singularity executable. {volumes} are the mount points that the runner automatically constructs based upon the task input/output specifications. {image_path} is the path to Singularity image ( {image_dir}/{image} ). {task_args} is the task command line arguments, constructed automatically by the runner.","title":"Running MLCubes"},{"location":"runners/ssh-runner/","text":"SSH Runner \u00b6 SSH runner uses other runners to run MLCube cubes on remote hosts. It uses ssh and rsync internally. It supports two mandatory commands - configure and run with standard arguments - mlcube , platform and task . Users can configure SSH runner in system setting file, and override parameters on a command line. Work in progress. Some functionality described below may not be available. Configuration parameters \u00b6 # Remote host name or IP address host : '' # Platform (runner) to use on remote host platform : '' # Root path for MLCubes on remote host remote_root : '' # Remote python interpreter. It's a dictionary. # - Must contain: # - `type`: interpreter type (system, virtualenv) # - When type is system (system-wide interpreter), additional parameters must be: # - `python`: python executable, maybe full path or just `python`. # - `requirements`: is a whitespace-separated list of python dependencies. # - When type is virtualenv (python environment created with virtualenv tool), # additional parameters must be: # - `python`: python executable # - `requirements`: is a whitespace-separated list of python dependencies. # - `location`: path where virtual environment must be created. # - `name`: name of the virtual environment. interpreter : {} # Authentication on remote host. It's a dictionary that contain the following fields: # - `identify_file`: if present, will be used as part of the connection # string ('-i {identity_file}') # - `user`: username for the remote host, will be used as '{user}@{host}' authentication : {} SSH runner uses IP or name of a remote host ( host ) and ssh tool to log in and execute shell commands on remote hosts. If passwordless login is not configured, SSH runner asks for password many times during configure and run phases. Configuring MLCubes \u00b6 This runner must be configured by users explicitly: mlcube configure --mlcube=. --platform=ssh During the configure phase, the following steps are performed. Based upon configuration, SSH runner creates and/or configures python on a remote host using ssh . This includes execution of such commands as virtualenv -p ... and/or source ... && pip install ... on a remote host. SSH runner copies mlcube directory to a remote host. SSH runner runs another runner specified in a platform configuration file on a remote host to configure it. Running MLCubes \u00b6 During the run phase, the SSH runner performs the following steps: It uses ssh to run standard run command on a remote host. It uses rsync to synchronize back the content of the {MLCUBE_ROOT}/workspace directory.","title":"SSH Runner"},{"location":"runners/ssh-runner/#ssh-runner","text":"SSH runner uses other runners to run MLCube cubes on remote hosts. It uses ssh and rsync internally. It supports two mandatory commands - configure and run with standard arguments - mlcube , platform and task . Users can configure SSH runner in system setting file, and override parameters on a command line. Work in progress. Some functionality described below may not be available.","title":"SSH Runner"},{"location":"runners/ssh-runner/#configuration-parameters","text":"# Remote host name or IP address host : '' # Platform (runner) to use on remote host platform : '' # Root path for MLCubes on remote host remote_root : '' # Remote python interpreter. It's a dictionary. # - Must contain: # - `type`: interpreter type (system, virtualenv) # - When type is system (system-wide interpreter), additional parameters must be: # - `python`: python executable, maybe full path or just `python`. # - `requirements`: is a whitespace-separated list of python dependencies. # - When type is virtualenv (python environment created with virtualenv tool), # additional parameters must be: # - `python`: python executable # - `requirements`: is a whitespace-separated list of python dependencies. # - `location`: path where virtual environment must be created. # - `name`: name of the virtual environment. interpreter : {} # Authentication on remote host. It's a dictionary that contain the following fields: # - `identify_file`: if present, will be used as part of the connection # string ('-i {identity_file}') # - `user`: username for the remote host, will be used as '{user}@{host}' authentication : {} SSH runner uses IP or name of a remote host ( host ) and ssh tool to log in and execute shell commands on remote hosts. If passwordless login is not configured, SSH runner asks for password many times during configure and run phases.","title":"Configuration parameters"},{"location":"runners/ssh-runner/#configuring-mlcubes","text":"This runner must be configured by users explicitly: mlcube configure --mlcube=. --platform=ssh During the configure phase, the following steps are performed. Based upon configuration, SSH runner creates and/or configures python on a remote host using ssh . This includes execution of such commands as virtualenv -p ... and/or source ... && pip install ... on a remote host. SSH runner copies mlcube directory to a remote host. SSH runner runs another runner specified in a platform configuration file on a remote host to configure it.","title":"Configuring MLCubes"},{"location":"runners/ssh-runner/#running-mlcubes","text":"During the run phase, the SSH runner performs the following steps: It uses ssh to run standard run command on a remote host. It uses rsync to synchronize back the content of the {MLCUBE_ROOT}/workspace directory.","title":"Running MLCubes"},{"location":"tutorials/create-mlcube/","text":"Tutorial: Create an MLCube \u00b6 Interested in getting started with MLCube? Follow the instructions in this tutorial. Step 1: Setup \u00b6 Get MLCube, MLCube examples and MLCube Templates, and CREATE a Python environment. # You can clone the mlcube examples and templates from GtiHub git clone https://github.com/mlcommons/mlcube_examples # Create a python environment virtualenv -p python3 ./env && source ./env/bin/activate # Install mlcube, mlcube-docker and cookiecutter pip install mlcube mlcube-docker cookiecutter Step 2: Configure MLCube using the mlcube_cookiecutter \u00b6 Let's use the matmul example, that we downloaded in the previous step, to illustrate how to make an MLCube. Matmul is a simple matrix multiply example written in Python with TensorFlow. When you create an MLCube for your own model you will use your own code, data and dockerfile. cd mlcube_examples # rename matmul reference implementation from matmul to matmul_reference mv ./matmul ./matmul_reference # create a mlcube directory using mlcube template(note: do not use quotes in your input to # cookiecutter): # mlcube_name = matmul # mlcube_description = Matrix multiplication example # author_name = MLPerf Best Practices Working Group # author_email = first.second@corp.com # author_org = corp cookiecutter https://github.com/mlcommons/mlcube_cookiecutter.git # copy the matmul.py,Dockerfile and requirements.txt to your mlcube_matmul/build directory cp matmul_reference/matmul.py matmul/ cp matmul_reference/Dockerfile matmul/ cp matmul_reference/requirements.txt matmul/ # copy input file for matmul to workspace directory cp -R matmul_reference/workspace matmul Edit the template files Start by looking at the mlcube.yaml file that has been generated by cookiecutter. cd ./matmul Cookiecutter has modified the lines shown in bold in the mlcube.yaml file shown here: # This YAML file marks a directory to be an MLCube directory. When running MLCubes with runners, # MLCube path is specified using `--mlcube` runner command line argument. The most important # parameters that are defined here are (1) name, (2) author and (3) list of MLCube tasks. # MLCube name (string). Replace it with your MLCube name (e.g. \"MNIST\"). name: matmul # MLCube description (string). Replace it with your MLCube name # (e.g. \"MLCommons MNIST MLCube example\"). description: Matrix multiplication example # List of authors. Cookiecutter sets the first author. authors: - name: MLPerf Best Practices Working Group email: first.second@corp.com org: corp # This dictionary can contain information about SW/HW requirements platform: {} # accelerator_count: 0 # accelerator_maker: NVIDIA # accelerator_model: A100-80GB # host_memory_gb: 40 # need_internet_access: True # host_disk_space_gb: 100 # This cookiecutter creates a docker-based MLCube. docker: image: mlcommons/ matmul :0.0.1 # List of MLCube tasks supported by this MLCube. tasks: matmul : parameters: inputs: {parameters_file: parameters_file.yaml} outputs: {output_file: {type: file, default: output.txt}} Our input file shapes.yaml that we have copied previously into the mlcube workspace contains input parameters to set matrix dimensions. We need to remove the automatically generated parameters file. rm workspace/parameters_file.yaml Now we will edit file mlcube.yaml. The lines you need to edit are shown in bold shown here: # This YAML file marks a directory to be an MLCube directory. When running MLCubes with runners, # MLCube path is specified using `--mlcube` runner command line argument. The most important # parameters that are defined here are (1) name, (2) author and (3) list of MLCube tasks. # MLCube name (string). Replace it with your MLCube name (e.g. \"MNIST\"). name: matmul # MLCube description (string). Replace it with your MLCube name # (e.g. \"MLCommons MNIST MLCube example\"). description: Matrix multiplication example # List of authors. Cookiecutter sets the first author. authors: - name: MLPerf Best Practices Working Group email: first.second@corp.com org: corp # This dictionary can contain information about SW/HW requirements platform: {} # accelerator_count: 0 # accelerator_maker: NVIDIA # accelerator_model: A100-80GB # host_memory_gb: 40 # need_internet_access: True # host_disk_space_gb: 100 # This cookiecutter creates a docker-based MLCube. docker: image: mlcommons/matmul:v1.0 # List of MLCube tasks supported by this MLCube. tasks: matmul: parameters: inputs: {parameters_file: shapes.yaml } outputs: {output_file: {type: file, default: matmul_output.txt }} Step 3: Build Docker container Image \u00b6 mlcube configure --mlcube = . --platform = docker -Prunner.build_strategy = auto Step 4: Test your MLCube \u00b6 mlcube run --mlcube = . --platform = docker --task = matmul ls ./workspace cat ./workspace/matmul_output.txt","title":"How to Create an MLCube"},{"location":"tutorials/create-mlcube/#tutorial-create-an-mlcube","text":"Interested in getting started with MLCube? Follow the instructions in this tutorial.","title":"Tutorial: Create an MLCube"},{"location":"tutorials/create-mlcube/#step-1-setup","text":"Get MLCube, MLCube examples and MLCube Templates, and CREATE a Python environment. # You can clone the mlcube examples and templates from GtiHub git clone https://github.com/mlcommons/mlcube_examples # Create a python environment virtualenv -p python3 ./env && source ./env/bin/activate # Install mlcube, mlcube-docker and cookiecutter pip install mlcube mlcube-docker cookiecutter","title":"Step 1: Setup"},{"location":"tutorials/create-mlcube/#step-2-configure-mlcube-using-the-mlcube_cookiecutter","text":"Let's use the matmul example, that we downloaded in the previous step, to illustrate how to make an MLCube. Matmul is a simple matrix multiply example written in Python with TensorFlow. When you create an MLCube for your own model you will use your own code, data and dockerfile. cd mlcube_examples # rename matmul reference implementation from matmul to matmul_reference mv ./matmul ./matmul_reference # create a mlcube directory using mlcube template(note: do not use quotes in your input to # cookiecutter): # mlcube_name = matmul # mlcube_description = Matrix multiplication example # author_name = MLPerf Best Practices Working Group # author_email = first.second@corp.com # author_org = corp cookiecutter https://github.com/mlcommons/mlcube_cookiecutter.git # copy the matmul.py,Dockerfile and requirements.txt to your mlcube_matmul/build directory cp matmul_reference/matmul.py matmul/ cp matmul_reference/Dockerfile matmul/ cp matmul_reference/requirements.txt matmul/ # copy input file for matmul to workspace directory cp -R matmul_reference/workspace matmul Edit the template files Start by looking at the mlcube.yaml file that has been generated by cookiecutter. cd ./matmul Cookiecutter has modified the lines shown in bold in the mlcube.yaml file shown here: # This YAML file marks a directory to be an MLCube directory. When running MLCubes with runners, # MLCube path is specified using `--mlcube` runner command line argument. The most important # parameters that are defined here are (1) name, (2) author and (3) list of MLCube tasks. # MLCube name (string). Replace it with your MLCube name (e.g. \"MNIST\"). name: matmul # MLCube description (string). Replace it with your MLCube name # (e.g. \"MLCommons MNIST MLCube example\"). description: Matrix multiplication example # List of authors. Cookiecutter sets the first author. authors: - name: MLPerf Best Practices Working Group email: first.second@corp.com org: corp # This dictionary can contain information about SW/HW requirements platform: {} # accelerator_count: 0 # accelerator_maker: NVIDIA # accelerator_model: A100-80GB # host_memory_gb: 40 # need_internet_access: True # host_disk_space_gb: 100 # This cookiecutter creates a docker-based MLCube. docker: image: mlcommons/ matmul :0.0.1 # List of MLCube tasks supported by this MLCube. tasks: matmul : parameters: inputs: {parameters_file: parameters_file.yaml} outputs: {output_file: {type: file, default: output.txt}} Our input file shapes.yaml that we have copied previously into the mlcube workspace contains input parameters to set matrix dimensions. We need to remove the automatically generated parameters file. rm workspace/parameters_file.yaml Now we will edit file mlcube.yaml. The lines you need to edit are shown in bold shown here: # This YAML file marks a directory to be an MLCube directory. When running MLCubes with runners, # MLCube path is specified using `--mlcube` runner command line argument. The most important # parameters that are defined here are (1) name, (2) author and (3) list of MLCube tasks. # MLCube name (string). Replace it with your MLCube name (e.g. \"MNIST\"). name: matmul # MLCube description (string). Replace it with your MLCube name # (e.g. \"MLCommons MNIST MLCube example\"). description: Matrix multiplication example # List of authors. Cookiecutter sets the first author. authors: - name: MLPerf Best Practices Working Group email: first.second@corp.com org: corp # This dictionary can contain information about SW/HW requirements platform: {} # accelerator_count: 0 # accelerator_maker: NVIDIA # accelerator_model: A100-80GB # host_memory_gb: 40 # need_internet_access: True # host_disk_space_gb: 100 # This cookiecutter creates a docker-based MLCube. docker: image: mlcommons/matmul:v1.0 # List of MLCube tasks supported by this MLCube. tasks: matmul: parameters: inputs: {parameters_file: shapes.yaml } outputs: {output_file: {type: file, default: matmul_output.txt }}","title":"Step 2: Configure MLCube using the mlcube_cookiecutter"},{"location":"tutorials/create-mlcube/#step-3-build-docker-container-image","text":"mlcube configure --mlcube = . --platform = docker -Prunner.build_strategy = auto","title":"Step 3: Build Docker container Image"},{"location":"tutorials/create-mlcube/#step-4-test-your-mlcube","text":"mlcube run --mlcube = . --platform = docker --task = matmul ls ./workspace cat ./workspace/matmul_output.txt","title":"Step 4: Test your MLCube"}]}